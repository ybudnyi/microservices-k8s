API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2021-11-22T12:28:34Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
    Manager:         helm
    Operation:       Update
    Time:            2021-11-22T12:28:34Z
  Resource Version:  42174
  UID:               dfec4f84-1d4f-449a-9fbd-b7012e3e892f
Spec:
  Groups:
    Name:  alertmanager.rules
    Rules:
      Alert:  AlertmanagerFailedReload
      Annotations:
        Description:  Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerfailedreload
        Summary:      Reloading an Alertmanager configuration has failed.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
max_over_time(alertmanager_config_last_reload_successful{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}[5m]) == 0
      For:  10m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerMembersInconsistent
      Annotations:
        Description:  Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagermembersinconsistent
        Summary:      A member of an Alertmanager cluster has not found all other cluster members.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
  max_over_time(alertmanager_cluster_members{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}[5m])
< on (namespace,service) group_left
  count by (namespace,service) (max_over_time(alertmanager_cluster_members{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}[5m]))
      For:  15m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerFailedToSendAlerts
      Annotations:
        Description:  Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerfailedtosendalerts
        Summary:      An Alertmanager instance failed to send notifications.
      Expr:           (
  rate(alertmanager_notifications_failed_total{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}[5m])
/
  rate(alertmanager_notifications_total{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}[5m])
)
> 0.01
      For:  5m
      Labels:
        Severity:  warning
      Alert:       AlertmanagerClusterFailedToSendAlerts
      Annotations:
        Description:  The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerclusterfailedtosendalerts
        Summary:      All Alertmanager instances in a cluster failed to send notifications to a critical integration.
      Expr:           min by (namespace,service, integration) (
  rate(alertmanager_notifications_failed_total{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring", integration=~`.*`}[5m])
/
  rate(alertmanager_notifications_total{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring", integration=~`.*`}[5m])
)
> 0.01
      For:  5m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerClusterFailedToSendAlerts
      Annotations:
        Description:  The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerclusterfailedtosendalerts
        Summary:      All Alertmanager instances in a cluster failed to send notifications to a non-critical integration.
      Expr:           min by (namespace,service, integration) (
  rate(alertmanager_notifications_failed_total{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring", integration!~`.*`}[5m])
/
  rate(alertmanager_notifications_total{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring", integration!~`.*`}[5m])
)
> 0.01
      For:  5m
      Labels:
        Severity:  warning
      Alert:       AlertmanagerConfigInconsistent
      Annotations:
        Description:  Alertmanager instances within the {{$labels.job}} cluster have different configurations.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerconfiginconsistent
        Summary:      Alertmanager instances within the same cluster have different configurations.
      Expr:           count by (namespace,service) (
  count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"})
)
!= 1
      For:  20m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerClusterDown
      Annotations:
        Description:  {{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerclusterdown
        Summary:      Half or more of the Alertmanager instances within the same cluster are down.
      Expr:           (
  count by (namespace,service) (
    avg_over_time(up{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}[5m]) < 0.5
  )
/
  count by (namespace,service) (
    up{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}
  )
)
>= 0.5
      For:  5m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerClusterCrashlooping
      Annotations:
        Description:  {{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.
        runbook_url:  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerclustercrashlooping
        Summary:      Half or more of the Alertmanager instances within the same cluster are crashlooping.
      Expr:           (
  count by (namespace,service) (
    changes(process_start_time_seconds{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}[10m]) > 4
  )
/
  count by (namespace,service) (
    up{job="monitoring-kube-prometheus-alertmanager",namespace="monitoring"}
  )
)
>= 0.5
      For:  5m
      Labels:
        Severity:  critical
Events:            <none>
